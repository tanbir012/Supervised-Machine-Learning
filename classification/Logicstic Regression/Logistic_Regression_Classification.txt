LOGISTIC REGRESSION ALGORITHM FOR CLASSIFICATION

Logistic Regression is a statistical model used for binary classification. Despite its name, it is a classification algorithm rather than a regression algorithm. It models the probability of a certain class or event existing.

1.  **Core Idea:**
    Logistic Regression predicts a probability (between 0 and 1) that an instance belongs to a particular class (e.g., "yes" or "no", "true" or "false"). This probability is then converted into a binary class prediction based on a threshold (commonly 0.5).

2.  **Hypothesis Function (Sigmoid Function):**
    The core of Logistic Regression is the sigmoid (or logistic) function, which maps any real-valued number to a value between 0 and 1.

    * **Linear Combination (z):** For an input feature vector 'x' and parameters (weights 'w' and bias 'b'), we first calculate a linear combination:
        `z = w_1*x_1 + w_2*x_2 + ... + w_n*x_n + b`
        In vector form: `z = w^T * x + b`

    * **Sigmoid Transformation (h(x)):** This 'z' value is then passed through the sigmoid function:
        `h(x) = 1 / (1 + e^(-z))`
        This `h(x)` represents the predicted probability `P(Y=1 | x)`, i.e., the probability that the output 'Y' is 1 given the input 'x'.

3.  **Cost Function (Log-Loss / Binary Cross-Entropy):**
    Unlike Linear Regression which uses Mean Squared Error, Logistic Regression uses a cost function (loss function) that penalizes incorrect probabilistic predictions more heavily. The most common is Binary Cross-Entropy:

    * For a single training example (x, y), where 'y' is the actual label (0 or 1) and 'h(x)' is the predicted probability:
        If y = 1: `Cost = -log(h(x))`
        If y = 0: `Cost = -log(1 - h(x))`

    * This can be combined into a single formula for all 'm' training examples:
        `J(w, b) = - (1/m) * SUM [y_i * log(h(x_i)) + (1 - y_i) * log(1 - h(x_i))]`
        The goal during training is to minimize this cost function.

4.  **Optimization (Gradient Descent):**
    To minimize the cost function `J(w, b)`, we use an iterative optimization algorithm like Gradient Descent.

    * **Initialization:** Start with initial values for weights 'w' and bias 'b' (often zeros or small random numbers).

    * **Iteration:** Repeat for a number of epochs or until convergence:
        a.  **Calculate Predictions:** For all training examples, calculate `h(x_i)`.
        b.  **Calculate Gradients:** Compute the partial derivatives of the cost function with respect to 'w' and 'b'. These derivatives tell us the direction and magnitude of the steepest ascent of the cost function.
            * `dJ/dw_j = (1/m) * SUM [(h(x_i) - y_i) * x_ij]` (for each weight w_j)
            * `dJ/db = (1/m) * SUM [(h(x_i) - y_i)]`
        c.  **Update Parameters:** Adjust 'w' and 'b' in the opposite direction of the gradient, scaled by a learning rate `alpha`.
            * `w := w - alpha * dJ/dw`
            * `b := b - alpha * dJ/db`

5.  **Prediction (Classification):**
    Once the model is trained and optimal 'w' and 'b' are found:

    * For a new input `x_new`, calculate `h(x_new)`.
    * If `h(x_new) >= threshold` (e.g., 0.5), classify as Class 1.
    * If `h(x_new) < threshold`, classify as Class 0.

6.  **Key Assumptions/Considerations:**
    * **Binary Outcome:** Strictly for binary classification. For multi-class, extensions like One-vs-Rest (OvR) or Softmax Regression (Multinomial Logistic Regression) are used.
    * **Linear Relationship:** Assumes a linear relationship between the input features and the log-odds of the outcome.
    * **No Multicollinearity:** Independent variables should not be highly correlated with each other.
    * **Large Sample Size:** Generally performs better with larger datasets.
    * **Feature Scaling:** While not strictly necessary for convergence, scaling features can speed up gradient descent.

Logistic Regression is a fundamental and widely used algorithm due to its simplicity, interpretability, and effectiveness for many binary classification tasks.
